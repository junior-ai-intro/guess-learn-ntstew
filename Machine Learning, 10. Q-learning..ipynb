{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = 'center'>Guessing Games</h1>\n",
    "<h3 align = 'center'>machine learning, one step at a time</h3>\n",
    "<h3 align = 'center'>Step 10. Q-learning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Q-learning**\n",
    "\n",
    "_In Q-learning, a form of reinforcement learning, an agent develops an optimal policy based on interactions with an environment; the environment provides a series of state-action-reward sequences without any additional descriptions, labels, context, or rules._\n",
    "\n",
    "Our last attempt at solving the maze is almost a Q-learning algorithm. It developed the policy \"don't repeat your mistakes\" to find a path through the maze, but it was not nearly the _optimal_ policy. An _optimal_ policy would find the exit in the fewest possible steps. When we consider what we can learn from our states, actions, and rewards, we are missing something.\n",
    "\n",
    "Our last exercise was good at exploiting __penalties__, but what about exploiting __rewards__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Find the reward in this q-table:***\n",
    "<pre>\n",
    "=====  ================================\n",
    "state         N       S       E       W\n",
    "\n",
    "(0,0)      -839       0       0    -866\n",
    "(0,1)      -241    -283       0       0\n",
    "(0,2)       -67       0     -53       0\n",
    "(0,3)         0       0       0       0\n",
    "\n",
    "(1,0)         0       0    -224    -227\n",
    "(1,1)         0       0       0       0\n",
    "(1,2)         0     -16       0     -10\n",
    "(1,3)        -3       0      -6       0\n",
    "\n",
    "(2,0)         0     -73       0     -53\n",
    "(2,1)       -12     -12     -12       0\n",
    "(2,2)         0       0       0       0\n",
    "(2,3)         0      __+1__      -1      -1  < -- there is the +1, over in the second column!\n",
    "\n",
    "(3,0)         0       0       0       0\n",
    "(3,1)         0       0       0       0\n",
    "(3,2)         0       0       0       0\n",
    "(3,3)         0       0       0       0\n",
    "\n",
    " </pre>\n",
    "in this table, it took 3,000 random attempts to find the exit once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***There is something critically important hiding in that table.***\n",
    "\n",
    "The table cleary says that, staring in state (2,3), a move to the South results in the state (3,3) and a reward of +1. Let's make our own notation for that:\n",
    "<pre>\n",
    "(2,3)[S] >>> (3,3){+1}  (the >>> means \"transitions to\")\n",
    "</pre>\n",
    "That suggests that __state (2,3) is a pretty good place to be__, because from (2,3) we can acheive a positive reward. How could we use that knowledge to favor actions that put us in (2,3)?\n",
    "\n",
    "Let's look again at the maze, marked up to show __(2,3)[S] >>> (3,3){+1}__:\n",
    "<pre>\n",
    "         ...  ...  ...  +++ \n",
    "enter->  (1)  ...  ...  +++ \n",
    "         ...  ...  ...  +++ \n",
    "\n",
    "         ...  +++  ...  ... \n",
    "         ...  +++  ...  ... \n",
    "         ...  +++  ...  ... \n",
    "\n",
    "         ...  ...  +++  2,3 \n",
    "         ...  ...  +++  [S]  <-this is a good state\n",
    "         ...  ...  +++   +1 \n",
    "\n",
    "         +++  +++  ...  ... \n",
    "         +++  +++  ...  3,3  <-exit\n",
    "         +++  +++  ...  ... \n",
    "\n",
    "</pre>\n",
    "That view of the maze raises an interesting question: if we are exploring, how can we _exploit_ the reward that is available _if and when we arrive at state (2,3)?_\n",
    "\n",
    "Well... how do we ever arrive at state (2,3)? In this maze, the only way is: __(1,3)[S] >>> (2,3){0}__:\n",
    "<pre>\n",
    "         ...  ...  ...  +++ \n",
    "enter->  (1)  ...  ...  +++ \n",
    "         ...  ...  ...  +++ \n",
    "\n",
    "         ...  +++  ...  1,3 \n",
    "         ...  +++  ...  [S]  <-this is a totally boring, uninteresting state\n",
    "         ...  +++  ...  -0- \n",
    "\n",
    "         ...  ...  +++  2,3 \n",
    "         ...  ...  +++  [S]  <-this is a good state\n",
    "         ...  ...  +++   +1 \n",
    "\n",
    "         +++  +++  ...  ... \n",
    "         +++  +++  ...  3,3  <-exit\n",
    "         +++  +++  ...  ... \n",
    "</pre>\n",
    "Wait a minute... how can (1,3) be totally boring, if it can lead us to (2,3)?\n",
    "\n",
    "Let's take a closer look at (1,3) and (2,3), by generating a q-table that has one reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====  ================================\n",
      "state         N       S       E       W\n",
      "\n",
      "(0,0)      -523       0       0    -486\n",
      "(0,1)      -154    -123       0       0\n",
      "(0,2)       -36       0     -37       0\n",
      "(0,3)         0       0       0       0\n",
      "-----  --------------------------------\n",
      "(1,0)         0       0    -154    -141\n",
      "(1,1)         0       0       0       0\n",
      "(1,2)         0      -6       0     -11\n",
      "(1,3)        -4       0      -3       0\n",
      "-----  --------------------------------\n",
      "(2,0)         0     -36       0     -42\n",
      "(2,1)        -6      -8      -2       0\n",
      "(2,2)         0       0       0       0\n",
      "(2,3)         0       1       0       0\n",
      "-----  --------------------------------\n",
      "(3,0)         0       0       0       0\n",
      "(3,1)         0       0       0       0\n",
      "(3,2)         0       0       0       0\n",
      "(3,3)         0       0       0       0\n",
      "-----  --------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from maze import Maze\n",
    "maze = Maze()\n",
    "\n",
    "def sample(maze):\n",
    "    action = maze.sample()                    # this returns N,S,E,W\n",
    "    return maze.action_space().index(action)  # this converts to 0,1,2,3\n",
    "\n",
    "# build a q-table that finds the exit once\n",
    "q = np.zeros((4,4,4)) \n",
    "stop = False\n",
    "while not stop:\n",
    "    state = maze.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = sample(maze)                         \n",
    "        new_state, reward, done = maze.step(action)  \n",
    "        q[state[0]][state[1]][action] += reward \n",
    "        state = new_state \n",
    "        if reward > 0:\n",
    "            stop = True\n",
    "\n",
    "Maze.print_q(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at (2,3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(q[2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should have +1 as the second entry, indicating a reward moving [S] to (3,3), which is the exit.\n",
    "\n",
    "What about (1,3)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.  0. -3.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(q[1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should have 0 as the second entry, indicating no penalty or reward for moving [S] to (2,3).\n",
    "\n",
    "Let's say we are exploring the maze __after we have solved it once__, and we find our way to (1,3), and we randomly consider the action [S]. At that point, if we were to step [S] from (1,3), we could notice that our new state (2,3) has a maximum reward of +1... before we actually take that step.\n",
    "\n",
    "In other words, for any possible action, we could ask _what is the maximum known reward of the resulting state, if we were to proceed with that action?_\n",
    "\n",
    "Instead of taking note that __(1,3)[S] >>> (2,3){0}__, we could try __(1,3)[S] >>> (2,3){max(2,3)}__, which says _if we move South from (1,3), we can reasonably expect to take advantage of the maximum known reward available at the resulting state (2,3)_.\n",
    "\n",
    "It's like saying _from past experience, I know there are cookies in the kitchen, so if a move puts me in the kitchen, I can expect to get a cookie._\n",
    "\n",
    "What is the maximum known reward at state (2,3)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum known reward at (2,3) = 1.0\n"
     ]
    }
   ],
   "source": [
    "print('maximum known reward at (2,3) =', max(q[2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's +1... because we found the exit from (2,3) on a previous attempt. When we make notes about (1,3)[S], we should take that into account. We should exploit the rewards, not just the penalties.\n",
    "\n",
    "And once we do that, we will be able to discover that __(1,2)[E] >>> (1,3){max(1,3)}__, and so on, backwards, all the way to the starting point:\n",
    "<pre>\n",
    "\n",
    "=== SIMPLIFIED VIEW OF HOW IT COULD WORK ===\n",
    "\n",
    "         0,0  0,1  0,2  +++ \n",
    "enter->  [E]  [E]  [S]  +++ \n",
    "          +1   +1   +1  +++       by looking forward for previous\n",
    "                                  rewards, we could find a way to\n",
    "         ...  +++  1,2  1,3       propogate those rewards backwards\n",
    "         ...  +++  [E]  [S]       along the optimum path to the\n",
    "         ...  +++   +1   +1       solution...\n",
    "                                  ...or something like that.\n",
    "         ...  ...  +++  2,3       \n",
    "         ...  ...  +++  [S]\n",
    "         ...  ...  +++   +1 \n",
    "\n",
    "         +++  +++  ...  ... \n",
    "         +++  +++  ...  3,3  <-exit\n",
    "         +++  +++  ...  ... \n",
    "         \n",
    "=== IT IS ALMOST THIS SIMPLE IN PRACTICE ===\n",
    "</pre>\n",
    "(\"or something like that\" means there a few details yet to discover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "***Exercises***<p>\n",
    "Explore the maze by exploiting rewards and penalites:\n",
    "- Alter the program to cause a positive reward to be noted for state (0,0)[E] by taking into account the maximum known reward for each step.\n",
    "- Accumulate enough positive rewards to find an optimal solution to the maze.\n",
    "- Discover that this approach works sometimes, but not always, by running it several times\n",
    "- Why doesn't this always work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maze import Maze\n",
    "\n",
    "'''\n",
    "=== DO NOT CHANGE CODE STARTING HERE ====== THOU SHALT NOT PASS ================================\n",
    "'''\n",
    "maze = Maze()\n",
    "def sample(maze):\n",
    "    action = maze.sample()                   \n",
    "    return maze.action_space().index(action)\n",
    "\n",
    "# follow the optimum path in the q-table, but don't take more than 12 steps\n",
    "def walk(q):\n",
    "    maze = Maze()\n",
    "    state = maze.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done and steps <= 12:\n",
    "        state, reward, done = maze.step(np.argmax(q[state[0]][state[1]]))\n",
    "        print(steps, maze)\n",
    "        steps += 1\n",
    "'''\n",
    "=== AND ENDING HERE ======================= THOU SHALT NOT PASS ================================\n",
    "'''\n",
    "q = np.zeros((4,4,4))\n",
    "solved = False\n",
    "while not solved:\n",
    "    state = maze.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = sample(maze)                         \n",
    "        new_state, reward, done = maze.step(action) \n",
    "        \n",
    "        ########################################################################################\n",
    "        #                                                                                      #\n",
    "        #  YOUR CODE GOES HERE...                                                              #\n",
    "        #                                                                                      #\n",
    "        #  row = state[0], col = state[1]                                                      #\n",
    "        #  do the usual update: q[row][col][action] += reward                                  #\n",
    "        #  then... if the new_state is in bounds:                                              #\n",
    "        #      adjust q[row][col][action] by the maximum reward available at new_state         #\n",
    "        #                                                                                      #\n",
    "        ########################################################################################\n",
    " \n",
    "        state = new_state\n",
    "        solved = True if max(q[0][0]) > 0 else False\n",
    "\n",
    "Maze.print_q(q)\n",
    "walk(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

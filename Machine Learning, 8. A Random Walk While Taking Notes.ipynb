{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = 'center'>Guessing Games</h1>\n",
    "<h3 align = 'center'>machine learning, one step at a time</h3>\n",
    "<h3 align = 'center'>Step 8. A Random Walk While Taking Notes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. A random walk while taking notes.**\n",
    "\n",
    "When we took our random walk, we ignored everything that happened except for one event: stumbling accross the exit.\n",
    "\n",
    "What if we paid attention, and learned from our mistakes?\n",
    "\n",
    "Let's start thinking about the maze in terms of machine learning: _the art of accumulating knowledge by learning from mistakes_.\n",
    "\n",
    "We have already seen that, when we explore the maze, it gives us feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze import Maze\n",
    "\n",
    "maze = Maze()\n",
    "print(maze)\n",
    "for i in range(10):           # take several random walks through the maze\n",
    "    state = maze.reset()      # start each walk at the initial state\n",
    "    done = False\n",
    "    print('\\n=== walk number ' + str(i) + ' =======================================================')\n",
    "    while not done:\n",
    "        action = maze.sample()\n",
    "        initial_state = state\n",
    "        state, reward, done = maze.step(action)\n",
    "        print('started at:', initial_state, '| moved:', action, '| reward =',reward, '| done?', done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to learn from those results, we would need to _remember_ our rewards or penalties. We would need to store something, someplace... like taking notes in class (unless you don't take notes; if you don't take notes, it's like taking notes as if you did take notes).\n",
    "\n",
    "If we were doing this by hand, what would we write in our notebook?\n",
    "\n",
    "...well, we only know two things:\n",
    "- our __state__ (that is, our x,y position)\n",
    "- the result of taking an __action__ when starting from our __state__.\n",
    "\n",
    "For example, here is the result of moving North immediately, and the notes we might take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = maze.reset()\n",
    "print('NOTE: initial state =', initial_state, '| attempting to move North...')\\\n",
    "\n",
    "new_state, reward, done = maze.step('N')\n",
    "print('NOTE: new_state =', new_state, '| reward =', reward, '| done ?', done)\n",
    "\n",
    "print('NOTE: Moving North from (0,0) is a bad idea!')\n",
    "print('NOTE: Why is it bad? Because I got a penalty!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so there's that.\n",
    "\n",
    "Seems like it's important to associate ( state(0,0) + action(N) = bad idea ).\n",
    "\n",
    "_But equally important... there's no need to consider any context (nothing above refers specifically to a maze). A bad idea is a bad idea. It doesn't matter why it's a bad idea, or what specifically is bad about it. Machine learning does not code for explicit conditions (like stepping out of bounds versus stepping onto a blocked square); we are only concerned with state transitions and outcomes, in order to find our way toward a goal or solution._\n",
    "\n",
    "That simplifies the problem. Who needs rules? We should be able simply to remember the result of the __transition__ associated with any __current state__ and __available action__ and go from there:\n",
    "<pre>\n",
    "== state =====   == action============    == transition =====    == result =============\n",
    "state is (0,0) + action is: move North -> new state is (0,-1) -> reward = -1, game over!\n",
    "==============   =====================    ===================    =======================\n",
    "</pre>\n",
    "\n",
    "We need a place to put all of our __states__, __actions__, and __rewards__ so that we can store the effect of any given __transition__.\n",
    "\n",
    "To do that, we just need the _dimensions_ of the problem... like putting a dozen eggs into a carton that is 6x2, or those occasional have-to-be-different cartons that are 4x3.\n",
    "\n",
    "The maze will reveal two things that will help us to discover the _dimensions_ of the maze problem: (1) the size of the __action__ space, and the size of the __state__ space. These size are __independent of the fact that it's a maze__. We could be solving any two-dimensional problem with multiple actions (like playing tic-tac-toe, or checkers). We don't care what the __actions__ or __states__ represent; we just need to know: how many are there?\n",
    "\n",
    "Remember: the goal is to solve the problem knowing as little as possible about the context... it's not maze, it's just an orderly set of states and transitions.\n",
    "\n",
    "(please re-read that last sentence)\n",
    "\n",
    "On to finding the _dimensions_: the set of dimensions that we have seen before is the __action space__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(maze.action_space())\n",
    "print('There are',len(maze.action_space()),'possible actions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the other set of dimensions, which our maze also provides, is the __state space__:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Here are the dimensions of all possible states:',maze.state_space())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE TO THE CURIOUS: it's not strictly necessary that the maze provide the dimensions of the action or state spaces. We could discover those dynamically, by exploring the problem over and over. Those are provided here just to simplify the example._\n",
    "\n",
    "Now we know that we have to deal with:\n",
    "- 4 actions\n",
    "- 4 x 4 = 16 states, that is really...\n",
    "    - 4 rows\n",
    "    - 4 columns\n",
    "\n",
    "We need to be able to remember the results of any of __action__ taken in any __state__, which makes 4x4x4, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np     # this library does all kinds of magical things with numbers\n",
    "q = np.zeros((4,4,4))  # don't worry about these details, just go with it\n",
    "print(q)               # everyone calls this a q-table... meaning the 'quality' of every action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a little hard to visualize which dimension is which... in detail, it works like this:\n",
    "<pre>\n",
    "row  col     --actions--\n",
    " 0    0   [[[N. S. E. W.]\n",
    " 0    1     [N. S. E. W.]\n",
    " 0    2     [N. S. E. W.]\n",
    " 0    3     [N. S. E. W.]]\n",
    "\n",
    " 1    0    [[N. S. E. W.]\n",
    " 1    1     [N. S. E. W.]\n",
    " 1    2     [N. S. E. W.]\n",
    " 1    3     [N. S. E. W.]]\n",
    " \n",
    " 2    0    [[N. S. E. W.]\n",
    " 2    1     [N. S. E. W.]\n",
    " 2    2     [N. S. E. W.]\n",
    " 2    3     [N. S. E. W.]]\n",
    " \n",
    " 3    0    [[N. S. E. W.]\n",
    " 3    1     [N. S. E. W.]\n",
    " 3    2     [N. S. E. W.]\n",
    " 3    3     [N. S. E. W.]]]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and yes, you could equivalently do this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.zeros((16,4)))  # around 1/2 the people find this easier to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we will stick with 4x4x4 for now.\n",
    "\n",
    "\n",
    "Let's say we want to remember that going North right away is a bad idea... start by moving North, then store the results in the __q-table__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze import Maze\n",
    "maze = Maze()\n",
    "initial_state = maze.reset()                 # start in state (0,0)\n",
    "final_state, reward, done = maze.step('N')   # take a step to the North\n",
    "print('started at:', initial_state, ', moved: N to ', final_state, ', reward =',reward, ', done?', done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means _when I was in state (0,0), and chose action 'N\", I got a reward of -1, and the game ended._\n",
    "\n",
    "Or, the _quality_ of the action 'N' from state (0,0) is pretty bad.\n",
    "\n",
    "Let's make a note of that in a 4x4x4 __q_table__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.zeros((4,4,4))\n",
    "row = 0                    # initial row is zero\n",
    "col = 0                    # initial col is zero\n",
    "action = 0                 # N,S,E,W = 0,1,2,3... so N = 0\n",
    "q[row][col][action] = -1   # remember that a bad thing happened\n",
    "print(q)                   # notice the -1 hiding in the upper left corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, let's convert our actions to numbers (so that later on we can store results in our __q-table__ automatically), like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a helpful function that you may need...\n",
    "# it converts N,S,E,W to 0,1,2,3\n",
    "\n",
    "def index_of_action(action):\n",
    "    return maze.action_space().index(action)\n",
    "\n",
    "# let's see how that works\n",
    "for action in maze.action_space():\n",
    "    print(action, index_of_action(action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store our penalty from moving North in our __q-table__ by hand using __index_of_action()__, we would do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q = np.zeros((4,4,4))\n",
    "q[0][0][index_of_action('N')] = -1   # we can put a function call inside an array index\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to store the __rewards__ or __penalties__ from every possible initial move, we would get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the row & col do not change...\n",
    "# these are the results of 4 transitions,\n",
    "# all starting from row = 0, col = 0.\n",
    "q[0][0][index_of_action('N')] = -1\n",
    "q[0][0][index_of_action('S')] = 0\n",
    "q[0][0][index_of_action('E')] = 0\n",
    "q[0][0][index_of_action('W')] = -1\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This q-table says 'hey, if you are just starting out... don't go north or west!'\n",
    "<hr>\n",
    "***Exercises***<p>\n",
    "    \n",
    "- Build a q-table that learns incrementally from 100,000 walks through the maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maze import Maze\n",
    "maze = Maze()\n",
    "\n",
    "# this converts N,S,E,W to 0,1,2,3\n",
    "def index_of_action(action):\n",
    "    return maze.action_space().index(action)\n",
    "\n",
    "q = np.zeros((4,4,4))\n",
    "for n in range(100000):\n",
    "    state = maze.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        ####################################################################\n",
    "        #                                                                  #\n",
    "        #  YOUR CODE HERE:                                                 #\n",
    "        #    - get a sample action from the maze                           #\n",
    "        #    - use the action to take a step; capture the return values    #\n",
    "        #    - update the q table by adding the reward to:                 #\n",
    "        #        > row = state[0]                                          #\n",
    "        #        > col = state[1]                                          #\n",
    "        #        > action = index_of_action(whatever action you took)      #\n",
    "        #        > q[row][col][action] += reward                           #\n",
    "        #     - and don't forget to update your state                      #\n",
    "        #                                                                  #\n",
    "        ####################################################################\n",
    "\n",
    "print(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = 'center'>Guessing Games</h1>\n",
    "<h3 align = 'center'>machine learning, one step at a time</h3>\n",
    "<h3 align = 'center'>Step 8. A Random Walk While Taking Notes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. A random walk while taking notes.**\n",
    "\n",
    "When we took our random walk, we ignored everything that happened except for one event: stumbling accross the exit.\n",
    "\n",
    "What if we paid attention, and learned from our mistakes?\n",
    "\n",
    "Let's start thinking about the maze in terms of machine learning: _the art of accumulating knowledge by learning from mistakes_.\n",
    "\n",
    "We have already seen that, when we explore the maze, it gives us feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze import Maze\n",
    "\n",
    "maze = Maze()\n",
    "print(maze)\n",
    "for i in range(10):           # take several random walks through the maze\n",
    "    state = maze.reset()      # start each walk at the initial state\n",
    "    done = False\n",
    "    print('\\n=== walk number ' + str(i) + ' =======================================================')\n",
    "    while not done:\n",
    "        action = maze.sample()\n",
    "        initial_state = state\n",
    "        state, reward, done = maze.step(action)\n",
    "        print('started at:', initial_state, '| moved:', action, '| reward =',reward, '| done?', done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to learn from those results, we would need to _remember_ our rewards or penalties. We would need to store something, someplace... like taking notes in class (unless you don't take notes; if you don't take notes, it's like taking notes as if you did take notes).\n",
    "\n",
    "If we were doing this by hand, what would we write in our notebook?\n",
    "\n",
    "...well, we only know two things:\n",
    "- our __state__ (that is, our x,y position)\n",
    "- the result of taking an __action__ when starting from our __state__.\n",
    "\n",
    "For example, here is the result of moving North immediately, and the notes we might take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = maze.reset()\n",
    "print('NOTE: initial state =', initial_state, '| attempting to move North...')\\\n",
    "\n",
    "new_state, reward, done = maze.step('N')\n",
    "print('NOTE: new_state =', new_state, '| reward =', reward, '| done ?', done)\n",
    "\n",
    "print('NOTE: Moving North from (0,0) is a bad idea!')\n",
    "print('NOTE: Why is it bad? Because I got a penalty!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so there's that.\n",
    "\n",
    "Seems like it's important to associate ( state(0,0) + action(N) = bad idea ).\n",
    "\n",
    "_But equally important... there's no need to consider any context (nothing above refers specifically to a maze). A bad idea is a bad idea. It doesn't matter why it's a bad idea, or what specifically is bad about it. Machine learning does not code for explicit conditions (like stepping out of bounds versus stepping onto a blocked square); we are only concerned with state transitions and outcomes, in order to find our way toward a goal or solution._\n",
    "\n",
    "That simplifies the problem. Who needs rules? We should be able simply to remember the result of the __transition__ associated with any __current state__ and __available action__ and go from there:\n",
    "<pre>\n",
    "== state =====   == action============    == transition =====    == result =============\n",
    "state is (0,0) + action is: move North -> new state is (0,-1) -> reward = -1, game over!\n",
    "==============   =====================    ===================    =======================\n",
    "</pre>\n",
    "\n",
    "We need a place to put all of our __states__, __actions__, and __rewards__ so that we can store the effect of any given __transition__.\n",
    "\n",
    "To do that, we just need the _dimensions_ of the problem... like putting a dozen eggs into a carton that is 6x2, or those occasional have-to-be-different cartons that are 4x3.\n",
    "\n",
    "The maze will reveal two things that will help us to discover the _dimensions_ of the maze problem: (1) the size of the __action__ space, and the size of the __state__ space. These size are __independent of the fact that it's a maze__. We could be solving any two-dimensional problem with multiple actions (like playing tic-tac-toe, or checkers). We don't care what the __actions__ or __states__ represent; we just need to know: how many are there?\n",
    "\n",
    "Remember: the goal is to solve the problem knowing as little as possible about the context... it's not maze, it's just an orderly set of states and transitions.\n",
    "\n",
    "(please re-read that last sentence)\n",
    "\n",
    "On to finding the _dimensions_: the set of dimensions that we have seen before is the __action space__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are',len(maze.action_space()),'possible actions:')\n",
    "print(maze.action_space())\n",
    "print('\\nWe might also refer to the actions by numerical value:')\n",
    "for n in range(len(maze.action_space())):\n",
    "    print(n,maze.action_space()[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the other set of dimensions, which our maze also provides, is the __state space__:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Here are the dimensions of all possible states:',maze.state_space())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE TO THE CURIOUS: it's not strictly necessary that the maze provide the dimensions of the action or state spaces. We could discover those dynamically, by exploring the problem over and over. Those are provided here just to simplify the example._\n",
    "\n",
    "Now we know that we have to deal with:\n",
    "- 4 actions\n",
    "- 4 x 4 = 16 states, that is really...\n",
    "    - 4 rows\n",
    "    - 4 columns\n",
    "\n",
    "We need to be able to remember the results of any of __action__ taken in any __state__, which makes 4x4x4, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np     # this library does all kinds of magical things with numbers\n",
    "q = np.zeros((4,4,4))  # don't worry about these details, just go with it\n",
    "print(q)               # everyone calls this a q-table... meaning the 'quality' of every action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a little hard to visualize which dimension is which... let's print it differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze import Maze\n",
    "Maze.print_q(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and yes, instead of using a 4x4x4 table, you could equivalently represent each of the 16 states as a single row, and do this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.zeros((16,4)))  # around 1/2 the people find this easier to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we will stick with 4x4x4 for now. If you want to use 16x4 in your exercises, that's OK.\n",
    "\n",
    "\n",
    "Let's say we want to remember that going North right away is a bad idea... start by moving North, then store the results in the __q-table__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze import Maze\n",
    "maze = Maze()\n",
    "initial_state = maze.reset()                 # start in state (0,0)\n",
    "final_state, reward, done = maze.step('N')   # take a step to the North\n",
    "print('started at:', initial_state, ', moved: N to ', final_state, ', reward =',reward, ', done?', done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means _when I was in state (0,0), and chose action 'N\", I got a reward of -1, and the game ended._\n",
    "\n",
    "Or, the _quality_ of the action 'N' from state (0,0) is pretty bad.\n",
    "\n",
    "Let's make a note of that in a 4x4x4 __q_table__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.zeros((4,4,4))\n",
    "row = 0                    # initial row is zero\n",
    "col = 0                    # initial col is zero\n",
    "action = 0                 # N,S,E,W = 0,1,2,3... so N = 0\n",
    "q[row][col][action] = -1   # remember that a bad thing happened at (0,0), action = 0 (North)\n",
    "Maze.print_q(q)            # notice the -1 hiding in the upper left corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to store the __rewards__ or __penalties__ from every possible initial move, we would get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These four entries refer to the initial state: (0,0)\n",
    "\n",
    "# R  C  A  = Row, Column, Action\n",
    "q[0][0][0] = -1   # Action = N = 0\n",
    "q[0][0][1] =  0   # Action = S = 1\n",
    "q[0][0][2] =  0   # Action = E = 2\n",
    "q[0][0][3] = -1   # Action = W = 3\n",
    "Maze.print_q(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This q-table says 'hey, if you are just starting out... don't go north or west!'\n",
    "<hr>\n",
    "***Exercises***<p>\n",
    "    \n",
    "- Build a q-table that learns incrementally from 100,000 walks through the maze.\n",
    "\n",
    "Before you begin, notice that the maze can provide random sample actions in either of two forms: __sample()__ provides actions as text (N,S,E,W); __sample_n()__ provides actions as numbers (0,1,2,3); the functins are interchangeable (use whichever form is more convenient):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maze import Maze\n",
    "\n",
    "##################################################\n",
    "#                                                #\n",
    "#   Run this to see the equivalence between      #\n",
    "#       maze.sample()                            #\n",
    "#       maze.sample_n()                          #\n",
    "#                                                #\n",
    "##################################################\n",
    "\n",
    "maze = Maze()\n",
    "print('Here are sample actions in text form')\n",
    "for n in range(5):\n",
    "    print(maze.sample())\n",
    "print('\\nHere are sample actions in numeric form')  \n",
    "for n in range(5):\n",
    "    print(maze.sample_n())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Here is the actual exercise... you could complete it using either form of sampling the maze:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maze import Maze\n",
    "\n",
    "maze = Maze()\n",
    "q = np.zeros((4,4,4))\n",
    "for n in range(100000):\n",
    "    state = maze.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        ####################################################################\n",
    "        #                                                                  #\n",
    "        #  YOUR CODE HERE:                                                 #\n",
    "        #    - get a sample action from the maze                           #\n",
    "        #    - use the action to take a step; capture the return values    #\n",
    "        #    - update the q table by adding the reward to:                 #\n",
    "        #        > row = state[0]                                          #\n",
    "        #        > col = state[1]                                          #\n",
    "        #        > action = numerical value of whatever action you took    #\n",
    "        #        > q[row][col][action] += reward                           #\n",
    "        #     - and don't forget to update your state                      #\n",
    "        #                                                                  #\n",
    "        ####################################################################\n",
    "\n",
    "Maze.print_q(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
